{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ycyuki/VR-project/blob/main/Image_based_Prediction_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vCcVqGoIjO69"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ySPfGAujf7GS"
      },
      "outputs": [],
      "source": [
        "# Define the F1 score metric\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "xQ9uiXWBFqZB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeCX_HzyjahR",
        "outputId": "77f9c466-ee7f-41c0-c69a-0ce4d4ac05ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ycgbDy8kJrq"
      },
      "outputs": [],
      "source": [
        "!! do not run this cell if the dataset is already create\n",
        "# Spilt the dataset into train, validation\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import shutil\n",
        "random.seed(42)\n",
        "# Set the base directory path\n",
        "new_base_dir = pathlib.Path('/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Train')\n",
        "\n",
        "# Set the paths for the raw data folders\n",
        "unstable_dir = new_base_dir / 'Unstable'\n",
        "stable_dir = new_base_dir / 'Stable'\n",
        "\n",
        "# Set the paths for the new data folders\n",
        "data_dir = new_base_dir.parent / 'data'\n",
        "train_dir = data_dir / 'train'\n",
        "val_dir = data_dir / 'val'\n",
        "\n",
        "# Create the new data folders if they don't exist\n",
        "(train_dir / 'unstable').mkdir(parents=True, exist_ok=True)\n",
        "(val_dir / 'unstable').mkdir(parents=True, exist_ok=True)\n",
        "(train_dir / 'stable').mkdir(parents=True, exist_ok=True)\n",
        "(val_dir / 'stable').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Function to split the data\n",
        "def split_data(source_dir, train_dir, val_dir, train_ratio=0.85, val_ratio=0.15):\n",
        "    files = os.listdir(source_dir)\n",
        "    if not files:\n",
        "        print(f\"The {source_dir} directory is empty. Skipping...\")\n",
        "        return\n",
        "\n",
        "    random.shuffle(files)\n",
        "    num_files = len(files)\n",
        "    train_count = int(num_files * train_ratio)\n",
        "    val_count = int(num_files * val_ratio)\n",
        "\n",
        "    for i, file in enumerate(files):\n",
        "        src_path = source_dir / file\n",
        "\n",
        "        if i < train_count:\n",
        "            dest_dir = train_dir\n",
        "        elif i < train_count + val_count:\n",
        "            dest_dir = val_dir\n",
        "        else:\n",
        "            dest_dir = test_dir\n",
        "\n",
        "        dest_path = dest_dir / file\n",
        "        shutil.copy(src_path, dest_path)\n",
        "\n",
        "# Split the unstable images\n",
        "split_data(unstable_dir, train_dir / 'unstable', val_dir / 'unstable')\n",
        "\n",
        "# Split the stable images\n",
        "split_data(stable_dir, train_dir / 'stable', val_dir / 'stable')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWhspXgt2kFW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "# Count the number of files in each subdirectory\n",
        "data_dir = Path('/content/drive/MyDrive/793Project/Tower_photo/Newmodel/data')\n",
        "\n",
        "for folder in data_dir.iterdir():\n",
        "    print(f\"Directory: {folder.name}\")\n",
        "    for sub_folder in folder.iterdir():\n",
        "        file_count = len(list(sub_folder.iterdir()))\n",
        "        print(f\"  Subdirectory: {sub_folder.name}, Number of files: {file_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwPS7tSD2soT",
        "outputId": "d22c37e6-44b9-40cc-b6f2-7ac6f364b4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 224 files belonging to 2 classes.\n",
            "Found 38 files belonging to 2 classes.\n",
            "Found 44 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Set the base directory path\n",
        "data_dir = Path('/content/drive/MyDrive/793Project/Tower_photo/Newmodel/data')\n",
        "# Set the paths for the new data folders\n",
        "train_dir = data_dir / 'train'\n",
        "val_dir = data_dir / 'val'\n",
        "test_dir = Path('/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test')\n",
        "\n",
        "# Load the datasets\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    shuffle=True,\n",
        "    image_size=(644, 394),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    shuffle=True,\n",
        "    image_size=(644, 394),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    shuffle=True,\n",
        "    image_size=(644, 394),\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngeiLROJj9NI"
      },
      "source": [
        "# Use ResNet50 pre-train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WqrnPcjakCEg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7CYUj98kDnE",
        "outputId": "0395559f-d355-4310-ab03-958d315240d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovRQ9JLCkOlf"
      },
      "outputs": [],
      "source": [
        "# Freeze the layers of the base_model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FljHKLt3kZmq"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), F1Score()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66ViZuuZkdrM"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=val_ds\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxyl1LnK6mXA"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVDsoZpB6ogN"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test dataset without shuffling for prediction clarity\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    shuffle=False,  # Disable shuffle for prediction phase\n",
        "    image_size=(644, 394),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Predict with the model\n",
        "predictions = model.predict(test_ds)\n",
        "\n",
        "# Get the predicted class labels with the highest probability\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get the maximum probability for each prediction (confidence of prediction)\n",
        "predicted_probabilities = np.max(predictions, axis=1)\n",
        "\n",
        "# Evaluate the model's performance on the test dataset\n",
        "evaluation = model.evaluate(test_ds)\n",
        "\n",
        "# Print the results for each image with their file paths\n",
        "file_paths = test_ds.file_paths\n",
        "for i, (prob, label) in enumerate(zip(predicted_probabilities, predicted_labels)):\n",
        "    print(f\"Image {i} ({file_paths[i]}): Predicted Label = {label}, Probability = {prob:.4f}\")\n",
        "\n",
        "# Print detailed probabilities for each class per image\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(f\"Image {i} ({file_paths[i]}):\")\n",
        "    for label_index, prob in enumerate(prediction):\n",
        "        print(f\"  Label {label_index}: Probability = {prob:.4f}\")\n",
        "\n",
        "# Print a sample of file paths\n",
        "print(\"Sample file paths:\", file_paths[:5])\n"
      ],
      "metadata": {
        "id": "ESBZ9C6nGwNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grad-CAM heatmap for ResNet50 model"
      ],
      "metadata": {
        "id": "f8vNmqCZHN8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions, ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm"
      ],
      "metadata": {
        "id": "fCE5dxEYHXZO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i3-JUEOO_D4_"
      },
      "outputs": [],
      "source": [
        "# Function to generate GradCAM heatmap\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=[model.inputs],\n",
        "        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        print(\"Conv outputs shape:\", conv_outputs.shape)\n",
        "        print(\"Predictions shape:\", predictions.shape)\n",
        "\n",
        "        if pred_index is None:\n",
        "            predictions = tf.keras.layers.GlobalAveragePooling2D()(predictions)\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "            pred_index = int(pred_index.numpy())\n",
        "\n",
        "        class_channel = predictions[0, pred_index]\n",
        "\n",
        "        grads = tape.gradient(class_channel, conv_outputs)\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "        conv_outputs = conv_outputs[0]\n",
        "        heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "        heatmap = tf.squeeze(heatmap)\n",
        "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# Function to process image\n",
        "def process_image(img_path, size=(644, 394)):\n",
        "    img = image.load_img(img_path, target_size=size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return preprocess_input(img_array)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image paths for testing\n",
        "img_paths = ['/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_1_Rotation_22.5_2024-04-03_13-02-43.png', # prediction: stable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_47_Rotation_135_2024-04-03_13-30-14.png', # prediction: stable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_47_Rotation_157.5_2024-04-03_13-30-15.png', # prediction: stable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_47_Rotation_315_2024-04-03_13-30-19.png', # prediction: unstable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Unstable/Tower_13_Rotation_135_2024-04-03_13-09-14.png', # prediction: unstable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Unstable/Tower_71_Rotation_135_2024-04-03_13-40-58.png', # prediction: stable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Unstable/Tower_71_Rotation_225_2024-04-03_13-41-00.png', # prediction: unstable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Unstable/Tower_71_Rotation_315_2024-04-03_13-41-01.png', # prediction: stable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Unstable/Tower_71_Rotation_45_2024-04-03_13-40-56.png', # prediction: stable\n",
        "             '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Unstable/Tower_13_Rotation_315_2024-04-03_13-09-18.png' # prediction: unstable\n",
        "             ]\n",
        "\n",
        "# Model details\n",
        "last_conv_layer_name = 'conv5_block2_out' # Name of the convolutional layer that will be used for GradCAM\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(644, 394, 3))\n",
        "resnet50_model_gradcam = base_model\n",
        "\n",
        "\n",
        "# Process each image and compute Grad-CAM heatmap\n",
        "for img_path in img_paths:\n",
        "    img_array = process_image(img_path)\n",
        "    heatmap = make_gradcam_heatmap(img_array, resnet50_model_gradcam, last_conv_layer_name)\n",
        "\n",
        "    # Read and prepare the image\n",
        "    color_img = cv2.imread(img_path)\n",
        "    color_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.cvtColor(color_img, cv2.COLOR_RGB2GRAY)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # Rescale heatmap to a range 0-255 and apply colormap\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    colored_heatmap = cm.jet(heatmap)[:,:,:3]\n",
        "    colored_heatmap = cv2.resize(colored_heatmap, (img.shape[1], img.shape[0]))\n",
        "    colored_heatmap = np.uint8(255 * colored_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on the original image\n",
        "    alpha = 0.6  # Heatmap transparency\n",
        "    superimposed_img = colored_heatmap * alpha + img * (1 - alpha)\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255)\n",
        "\n",
        "    # Set up Matplotlib figure and axes\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Display original image\n",
        "    ax[0].imshow(color_img)\n",
        "    ax[0].axis('off')\n",
        "    ax[0].set_title('Original Image')\n",
        "\n",
        "    # Display the image with the superimposed heatmap\n",
        "    ax[1].imshow(superimposed_img.astype('uint8'))\n",
        "    ax[1].axis('off')\n",
        "    ax[1].set_title('Image with Grad-CAM')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PYSHtEpRIFdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swSDPGRnACw-"
      },
      "source": [
        "# Use InceptionV3 pre-train model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "metadata": {
        "id": "HwO6y70ZKNqp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOdCGIs8ijN3"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    resize_and_rescale = tf.keras.Sequential([\n",
        "        tf.keras.layers.experimental.preprocessing.Resizing(299, 299),\n",
        "        tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "    ])\n",
        "\n",
        "    def apply_preprocessing(x, y):\n",
        "        return resize_and_rescale(x), y\n",
        "\n",
        "    return dataset.map(apply_preprocessing)\n",
        "\n",
        "# Preprocess the datasets\n",
        "train_ds = preprocess_dataset(train_ds)\n",
        "val_ds = preprocess_dataset(val_ds)\n",
        "test_ds = preprocess_dataset(test_ds)\n",
        "\n",
        "# Configure datasets for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "i_train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "i_val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "i_test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Load InceptionV3 model pre-trained on ImageNet data\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                               weights='imagenet',\n",
        "                                               input_shape=(299, 299, 3))\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model on top of the base model\n",
        "inputs = tf.keras.Input(shape=(299, 299, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "i_model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkPOhSAKOApr"
      },
      "outputs": [],
      "source": [
        "# Print model summary\n",
        "i_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "i_model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), F1Score()])\n"
      ],
      "metadata": {
        "id": "4vrypkbgJMnU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suWl2woRivlx"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = i_model.fit(i_train_ds,\n",
        "                    epochs=10,\n",
        "                    validation_data=i_val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMoRaBqdjAZr"
      },
      "outputs": [],
      "source": [
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkQL6Gqci0TU"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "i_model.evaluate(i_test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYp9yrVm_hvK"
      },
      "outputs": [],
      "source": [
        "# Predict the test dataset\n",
        "predictions = i_model.predict(i_test_ds)\n",
        "\n",
        "# Get the predicted class labels with the highest probability\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get the maximum probability for each prediction (confidence of prediction)\n",
        "predicted_probabilities = np.max(predictions, axis=1)\n",
        "\n",
        "# Evaluate the model's performance on the test dataset overall\n",
        "evaluation = i_model.evaluate(i_test_ds)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Model evaluation: {evaluation}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image paths for testing\n",
        "img_paths = [\n",
        "    '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_1_Rotation_0_2024-04-03_13-02-42.png',\n",
        "    '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_1_Rotation_112.5_2024-04-03_13-02-45.png',\n",
        "     '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_47_Rotation_0_2024-04-03_13-30-11.png',\n",
        "    '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_47_Rotation_112.5_2024-04-03_13-30-14.png',\n",
        "    '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Unstable/Tower_93_Rotation_135_2024-04-03_13-49-05.png',\n",
        "    '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Unstable/Tower_93_Rotation_225_2024-04-03_13-49-07.png'\n",
        "]\n",
        "\n",
        "# Function to load and preprocess image\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(299, 299))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return preprocess_input(img_array)\n",
        "\n",
        "# Load and preprocess all images\n",
        "images = np.vstack([load_and_preprocess_image(img_path) for img_path in img_paths])\n",
        "\n",
        "# Predict on individual images\n",
        "predictions = i_model.predict(images)\n",
        "\n",
        "# Get predicted labels and probabilities for individual images\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "predicted_probabilities = np.max(predictions, axis=1)\n",
        "\n",
        "# Print predictions for each image\n",
        "for i, (prob, label) in enumerate(zip(predicted_probabilities, predicted_labels)):\n",
        "    print(f\"Image {i}: Predicted Label = {label}, Probability = {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "iKLl00OuKrx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GradCAM heatmap for InceptionV3 model"
      ],
      "metadata": {
        "id": "mSjqr9TvKtLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.cm as cm\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9VPpMRMULRLH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "A5Lsf3uCOMkX"
      },
      "outputs": [],
      "source": [
        "# Define Grad-CAM function\n",
        "def make_gradcam_heatmap(img_array, base_model, last_conv_layer, pred_index=None):\n",
        "    # Create a new Grad-CAM model\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=base_model.inputs,\n",
        "        outputs=[last_conv_layer.output, base_model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0], axis=-1)\n",
        "        pred_index_flat = tf.reshape(pred_index, [-1])\n",
        "        one_hot = tf.one_hot(pred_index_flat, depth=preds.shape[-1])\n",
        "        preds = tf.reshape(preds, (-1, preds.shape[-1]))\n",
        "        class_channel = tf.matmul(preds, one_hot, transpose_b=True)\n",
        "        grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "        last_conv_layer_output = last_conv_layer_output[0]\n",
        "        heatmap = tf.reduce_sum(last_conv_layer_output * tf.expand_dims(tf.expand_dims(pooled_grads, axis=0), axis=0), axis=-1)\n",
        "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlvtxUhqOO2I"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess image\n",
        "img_path = '/content/drive/MyDrive/793Project/Tower_photo/Newmodel/Test/Stable/Tower_1_Rotation_0_2024-04-03_13-02-42.png'\n",
        "img = image.load_img(img_path, target_size=(299, 299))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "# Load base model\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(299, 299, 3))\n",
        "last_conv_layer = base_model.get_layer('mixed10')\n",
        "\n",
        "# Generate Grad-CAM heatmap\n",
        "heatmap = make_gradcam_heatmap(img_array, base_model, last_conv_layer)\n",
        "\n",
        "# Load the original image using cv2\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# Rescale heatmap to a range 0-255\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# Use jet colormap to colorize heatmap\n",
        "jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "# Use RGB values of the colormap\n",
        "jet_colors = jet(np.arange(256))[:, :3]\n",
        "jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "# Create an image with RGB colorized heatmap\n",
        "jet_heatmap = cv2.resize(jet_heatmap, (img.shape[1], img.shape[0]))\n",
        "jet_heatmap = np.uint8(255 * jet_heatmap)\n",
        "superimposed_img = jet_heatmap * 0.4 + img\n",
        "\n",
        "# Display the heatmap\n",
        "plt.imshow(superimposed_img / 255)\n",
        "plt.axis('off')\n",
        "plt.title('Grad-CAM Heatmap')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNr4EqzqwQWXhfAUewc0ev0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}